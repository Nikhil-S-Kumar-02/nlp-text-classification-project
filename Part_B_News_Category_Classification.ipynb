{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3ebaefb-2c4f-4da1-a1ed-90dbaaef9886",
   "metadata": {},
   "source": [
    "Video Link: https://drive.google.com/file/d/1OuvsmREwmC-aHIJ0lY_6yaXd2KJINuxf/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dafaa4a4-53e3-4e47-b436-535ce9a38cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   category                                           headline  \\\n",
      "0  WELLNESS              143 Miles in 35 Days: Lessons Learned   \n",
      "1  WELLNESS       Talking to Yourself: Crazy or Crazy Helpful?   \n",
      "2  WELLNESS  Crenezumab: Trial Will Gauge Whether Alzheimer...   \n",
      "3  WELLNESS                     Oh, What a Difference She Made   \n",
      "4  WELLNESS                                   Green Superfoods   \n",
      "\n",
      "                                               links  \\\n",
      "0  https://www.huffingtonpost.com/entry/running-l...   \n",
      "1  https://www.huffingtonpost.com/entry/talking-t...   \n",
      "2  https://www.huffingtonpost.com/entry/crenezuma...   \n",
      "3  https://www.huffingtonpost.com/entry/meaningfu...   \n",
      "4  https://www.huffingtonpost.com/entry/green-sup...   \n",
      "\n",
      "                                   short_description  \\\n",
      "0  Resting is part of training. I've confirmed wh...   \n",
      "1  Think of talking to yourself as a tool to coac...   \n",
      "2  The clock is ticking for the United States to ...   \n",
      "3  If you want to be busy, keep trying to be perf...   \n",
      "4  First, the bad news: Soda bread, corned beef a...   \n",
      "\n",
      "                             keywords  \n",
      "0                     running-lessons  \n",
      "1           talking-to-yourself-crazy  \n",
      "2  crenezumab-alzheimers-disease-drug  \n",
      "3                     meaningful-life  \n",
      "4                    green-superfoods  \n",
      "\n",
      "Dataset shape: (50000, 5)\n",
      "\n",
      "Class distribution:\n",
      " category\n",
      "WELLNESS          5000\n",
      "POLITICS          5000\n",
      "ENTERTAINMENT     5000\n",
      "TRAVEL            5000\n",
      "STYLE & BEAUTY    5000\n",
      "PARENTING         5000\n",
      "FOOD & DRINK      5000\n",
      "WORLD NEWS        5000\n",
      "BUSINESS          5000\n",
      "SPORTS            5000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      " category                0\n",
      "headline                0\n",
      "links                   0\n",
      "short_description       0\n",
      "keywords             2668\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data_news - data_news.csv')\n",
    "print(df.head())\n",
    "print(\"\\nDataset shape:\", df.shape)\n",
    "print(\"\\nClass distribution:\\n\", df['category'].value_counts())\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be7b2ce-199f-421e-ad47-22d36e6e28a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>143 Miles in 35 Days: Lessons Learned Resting ...</td>\n",
       "      <td>WELLNESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Talking to Yourself: Crazy or Crazy Helpful? T...</td>\n",
       "      <td>WELLNESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Crenezumab: Trial Will Gauge Whether Alzheimer...</td>\n",
       "      <td>WELLNESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oh, What a Difference She Made If you want to ...</td>\n",
       "      <td>WELLNESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Green Superfoods First, the bad news: Soda bre...</td>\n",
       "      <td>WELLNESS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category\n",
       "0  143 Miles in 35 Days: Lessons Learned Resting ...  WELLNESS\n",
       "1  Talking to Yourself: Crazy or Crazy Helpful? T...  WELLNESS\n",
       "2  Crenezumab: Trial Will Gauge Whether Alzheimer...  WELLNESS\n",
       "3  Oh, What a Difference She Made If you want to ...  WELLNESS\n",
       "4  Green Superfoods First, the bad news: Soda bre...  WELLNESS"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['keywords', 'links'], inplace=True)\n",
    "df['text'] = df['headline'] + ' ' + df['short_description']\n",
    "df[['text', 'category']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1332dbd0-a53a-4101-979d-6df170770eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nsk2k\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nsk2k\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nsk2k\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "C:\\Users\\nsk2k\\AppData\\Local\\Temp\\ipykernel_25436\\537859048.py:15: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mile day lesson learned resting part training ...</td>\n",
       "      <td>WELLNESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking crazy crazy helpful think talking tool...</td>\n",
       "      <td>WELLNESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>crenezumab trial gauge whether alzheimers drug...</td>\n",
       "      <td>WELLNESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oh difference made want busy keep trying perfe...</td>\n",
       "      <td>WELLNESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>green superfoods first bad news soda bread cor...</td>\n",
       "      <td>WELLNESS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  mile day lesson learned resting part training ...  WELLNESS\n",
       "1  talking crazy crazy helpful think talking tool...  WELLNESS\n",
       "2  crenezumab trial gauge whether alzheimers drug...  WELLNESS\n",
       "3  oh difference made want busy keep trying perfe...  WELLNESS\n",
       "4  green superfoods first bad news soda bread cor...  WELLNESS"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "df[\"clean_text\"] = df[\"text\"].apply(preprocess)\n",
    "df[[\"clean_text\", \"category\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b3242a-a63b-4289-8b0a-0a6f59ea6fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (50000, 5000)\n",
      "Number of categories: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df['clean_text'])\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['category'])\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Number of categories:\", len(label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c1b7bf6-361a-48aa-bcc8-0ec4a91e2e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7983\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      BUSINESS       0.73      0.78      0.75       955\n",
      " ENTERTAINMENT       0.77      0.78      0.78       985\n",
      "  FOOD & DRINK       0.85      0.82      0.84      1021\n",
      "     PARENTING       0.78      0.76      0.77      1030\n",
      "      POLITICS       0.79      0.74      0.76      1034\n",
      "        SPORTS       0.87      0.89      0.88       995\n",
      "STYLE & BEAUTY       0.86      0.85      0.85       986\n",
      "        TRAVEL       0.83      0.80      0.82      1008\n",
      "      WELLNESS       0.72      0.75      0.74      1009\n",
      "    WORLD NEWS       0.79      0.81      0.80       977\n",
      "\n",
      "      accuracy                           0.80     10000\n",
      "     macro avg       0.80      0.80      0.80     10000\n",
      "  weighted avg       0.80      0.80      0.80     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab202bcd-8f63-4eab-939b-4ac7da636dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc0289-6843-45c0-a5a6-e51665ba210f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
